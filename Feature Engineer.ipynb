{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tạo các features bậc 2 và tương quan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import numpy as np\n",
    "\n",
    "# Item đầu vào, thời gian, điểm cách biệt, rating player\n",
    "X = np.array([[12, -5, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4]])\n",
    "\n",
    "# Khởi tạo check xem các polynomial bậc 2 có cái nào dùng được không, không dùng bậc 3 vì chậm vcl =)))\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "print(\"Original Features Shape:\", X.shape)\n",
    "print(\"Transformed Features Shape:\", X_poly.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tạo thêm các biến có thể lấy từ playbyplay data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rating của đội\n",
    "team_avg_rating = X[:, 2:].mean(axis=1)\n",
    "#Tiền xử lý PlaybyPlay Data tạo ra thêm biến hiệp đấu\n",
    "#Normalize cách biệt điểm\n",
    "normalized_uppoints = X[:, 1] / (X[:, 0] + 1e-5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale lại các feature vì SVM nhạy hơn Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_poly)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Từ các features đã đưa vào, xem các feature nào có ít ảnh hưởng để loại ( Thực ra là làm cho có chứ ít features vcl )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "X_selected = selector.fit_transform(X_scaled)\n",
    "rfe = RFE(estimator=SVR(kernel=\"linear\"), n_features_to_select=10)\n",
    "X_rfe = rfe.fit_transform(X_selected, y)  #y là label hay target của tập training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check lại các feature ( Để thấy nó có ảnh hưởng thôi chứ chắc cũng ổn hết ;-;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_rfe, y) \n",
    "print(\"Feature Importances:\", model.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu đủ time thì automate bọn này để lấy dữ liệu live tự vào tự cập nhật (Pipeline Oách xà lách hỏi từ ChatGPT nhưng mà đéo biết có phải dùng Spark với Hadoop hay không ._. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('variance', VarianceThreshold(threshold=0.01)),\n",
    "    ('rfe', RFE(estimator=SVR(kernel=\"linear\"), n_features_to_select=10))\n",
    "])\n",
    "\n",
    "X_transformed = pipeline.fit_transform(X, y)  # Fit and transform\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
